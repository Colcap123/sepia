{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "pd.set_option('display.width', 10000)\n",
    "\n",
    "url = \"https://www.youtube.com/watch?v=wDAmezoNHJY\"\n",
    "\n",
    "vid_id = url.partition(\"/watch?v=\")[2]\n",
    "\n",
    "#retrieve the available transcripts\n",
    "transcript_list = YouTubeTranscriptApi.list_transcripts(vid_id)\n",
    "transcript = transcript_list.find_manually_created_transcript(['fr', 'fr-FR'])  \n",
    "if transcript != \"\":\n",
    "    df = pd.DataFrame(transcript.fetch())\n",
    "    #df.to_csv(\"transcript.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from yt_dlp import YoutubeDL\n",
    "from pydub import AudioSegment\n",
    "from subprocess import STDOUT, DEVNULL, call, run\n",
    "\n",
    "# Fetch best quality audio, re-encode to mono wav\n",
    "\n",
    "def fetch_encode_wav(url):\n",
    "    video_info = YoutubeDL().extract_info(\n",
    "        url = url,download=False\n",
    "    )\n",
    "\n",
    "    options={\n",
    "        'format':'bestaudio/best',\n",
    "        'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'wav',\n",
    "        'preferredquality': '320'\n",
    "        }],\n",
    "        'postprocessor_args': [\n",
    "            '-ar', '16000',\n",
    "            '-ac', '1'\n",
    "        ],\n",
    "        'prefer_ffmpeg': True,\n",
    "        'keepvideo':False,\n",
    "        'outtmpl':'audio/audio.wav',\n",
    "    }\n",
    "\n",
    "    with YoutubeDL(options) as ydl:\n",
    "        ydl.download([video_info['webpage_url']])\n",
    "\n",
    "    print(\"Download complete !\")\n",
    "\n",
    "    #-f s16le not supported by librosa !!!\n",
    "\n",
    "fetch_encode_wav(\"https://www.youtube.com/watch?v=wDAmezoNHJY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import run\n",
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os.path\n",
    "\n",
    "# Keep only clips containing specific word\n",
    "\n",
    "pd.set_option('display.width', 10000)\n",
    "\n",
    "#df = pd.read_csv(\"transcript.csv\")\n",
    "df.insert(2, 'stop', df['start'] + df['duration'], False)\n",
    "\n",
    "# Select rows with matching word regex\n",
    "df2 = df[df['text'].str.contains(r\"\\bphoto\\b\", regex=True)]\n",
    "print(f\"{df2.shape[0]} occurrence(s)\")\n",
    "#print(df2)\n",
    "# Make audio, generate SRT\n",
    "\n",
    "def cut_merge(cuts, audio_in, audio_out):\n",
    "    print(audio_in)\n",
    "    print(audio_out)\n",
    "    # Takes a list of tuples as cuts [(start, end)...]\n",
    "\n",
    "    cuts_old = len(cuts) \n",
    "    for i in range(len(cuts)):\n",
    "\n",
    "        if i < len(cuts)-1 and cuts[i][1] == cuts[i+1][0]:\n",
    "            cuts[i+1] = (cuts[i][0], cuts[i+1][1])\n",
    "            cuts.pop(i)\n",
    "\n",
    "    print(f\"total cuts : {len(cuts)}, removed {cuts_old - len(cuts)} useless cuts\") \n",
    "\n",
    "    # Opening file and extracting segment\n",
    "    segment = AudioSegment.from_wav(audio_in)\n",
    "    clips = []\n",
    "\n",
    "    for cut in cuts:\n",
    "        # cut_start, cut_end should be expressed in ms\n",
    "        clips.append(segment[cut[0]*1000:cut[1]*1000])\n",
    "\n",
    "    merged_clips = sum(clips)\n",
    "    merged_clips.export(audio_out, format=\"wav\")\n",
    "\n",
    "def rm_silence(audio_in, audio_out):\n",
    "\n",
    "    # remove silence with librosa\n",
    "    samples, sr = librosa.load(audio_in, mono=True, sr=None, duration=None, offset=0.0)\n",
    "    clips = librosa.effects.split(samples, top_db=30)\n",
    "\n",
    "    # Audio files should be normalized before removing silence, else manually set top_db everytime ???????\n",
    "\n",
    "    wav_data = []\n",
    "    for c in clips:\n",
    "        data = samples[c[0]: c[1]]\n",
    "        wav_data.extend(data)\n",
    "    sf.write(audio_out, wav_data, sr)\n",
    "\n",
    "cut_merge([(start_time,stop_time) for start_time,stop_time in zip(df2['start'], df2[\"stop\"])], \"audio/audio.wav\", \"audio/audio_clip.wav\")\n",
    "rm_silence(\"audio/audio_clip.wav\", \"audio/audio_clip_silenced.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vosk to get words timestamp\n",
    "\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "import sys\n",
    "import os\n",
    "import wave\n",
    "import subprocess\n",
    "import srt\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "SetLogLevel(-1)\n",
    "\n",
    "if not os.path.exists(\"model\"):\n",
    "    print (\"Please download the model from https://alphacephei.com/vosk/models and unpack as 'model' in the current folder.\")\n",
    "    exit (1)\n",
    "\n",
    "sample_rate=16000\n",
    "model = Model(\"model\")\n",
    "rec = KaldiRecognizer(model, sample_rate)\n",
    "rec.SetWords(True)\n",
    "audio_clip = \"audio/audio_clip.wav\"\n",
    "\n",
    "process = subprocess.Popen(['ffmpeg', '-loglevel', 'quiet', '-i',\n",
    "                            audio_clip,\n",
    "                            '-ar', str(sample_rate) , '-ac', '1', '-f', 's16le', '-'],\n",
    "                            stdout=subprocess.PIPE)\n",
    "\n",
    "WORDS_PER_LINE = 1\n",
    "\n",
    "def transcribe():\n",
    "    results = []\n",
    "    subs = []\n",
    "    while True:\n",
    "       data = process.stdout.read(4000)\n",
    "       if len(data) == 0:\n",
    "           break\n",
    "       if rec.AcceptWaveform(data):\n",
    "           results.append(rec.Result())\n",
    "    results.append(rec.FinalResult())\n",
    "\n",
    "    for i, res in enumerate(results):\n",
    "       jres = json.loads(res)\n",
    "       if not 'result' in jres:\n",
    "           continue\n",
    "       words = jres['result']\n",
    "       for j in range(0, len(words), WORDS_PER_LINE):\n",
    "           line = words[j : j + WORDS_PER_LINE] \n",
    "           s = srt.Subtitle(index=len(subs), \n",
    "                   content=\" \".join([l['word'] for l in line]),\n",
    "                   start=line[0]['start'], \n",
    "                   end=line[-1]['end'])\n",
    "           subs.append(s)\n",
    "    return subs\n",
    "\n",
    "words = transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for word in words:\n",
    "    if word.content == \"photo\":\n",
    "        labels.append((word.start, word.end))\n",
    "       \n",
    "cut_merge(labels, \"audio/audio_clip.wav\", \"audio/audio_final.wav\")\n",
    "print(f\"{len(labels)} words transcribed !\")\n",
    "print(\"playing final audio, turn volume up...\")\n",
    "final_segment = AudioSegment.from_wav(\"audio/audio_final.wav\")\n",
    "play(final_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVERAGE RUN TIME ~ 50s for (18-21)/26, 30 % error (transcribed/occurences)\n",
    "\n",
    "# Roughly 1 10min vid per minute, 60 vids / hours, if vid = 10min and transcribed = 5 : 60*5 = 300 new words per hour\n",
    "# Worse results when transcribing with unsilenced clip\n",
    "# Still other vosk models to try !"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e87eefdc7f02fbeaa81cb0507323209d5aeafeda9bc03c2b3b93122dcd6ddca"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('sepia-3.7.12': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
